{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### Character Level Language Model Using Wavenet Architecture Implemented From Scratch in PyTorch\n",
        "Paper link - https://arxiv.org/pdf/1609.03499\n",
        "\n",
        "Overview<br>\n",
        "1. Model takes in `block_size` number of characters as input and predicts the next character\n",
        "2. Input data is a list of names and output are new names. The data can be replaced with any large enough dataset (~100k tokens).\n",
        "3. Hyperparameters like `block_size`(context length), `emb_size`(embedding dimensions), `hidden_layer_dim`(hidden layer dimensions) can be tuned to achieve better performance"
      ],
      "metadata": {
        "id": "0drZyATSLKXd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MtJ-2Sj2Z4wy"
      },
      "outputs": [],
      "source": [
        "# import\n",
        "\n",
        "import torch.nn.functional as F\n",
        "import requests\n",
        "import random\n",
        "import torch\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset download\n",
        "\n",
        "names = requests.get(\"https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\").text.splitlines()"
      ],
      "metadata": {
        "id": "pSbZHuuCZ8z_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# string to token conversion\n",
        "\n",
        "itos = ['.'] + sorted(list(set(\"\".join(names))))\n",
        "stoi = {ch: idx for idx, ch in enumerate(itos)}"
      ],
      "metadata": {
        "id": "k1ApfqxWerD2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create dataset\n",
        "\n",
        "def create_data(words, block_size=3):\n",
        "    xs = []\n",
        "    ys = []\n",
        "    for word in words:\n",
        "        window = [0] * block_size\n",
        "        word += \".\"\n",
        "        for char in word:\n",
        "            xs.append(window)\n",
        "            ys.append(stoi[char])\n",
        "\n",
        "            window = window[1:]\n",
        "            window.append(stoi[char])\n",
        "    return torch.tensor(xs), torch.tensor(ys)"
      ],
      "metadata": {
        "id": "WDqUd9q1c2Dg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize train, val, test datasets\n",
        "\n",
        "# choose context length\n",
        "block_size = 8\n",
        "\n",
        "random.seed(17)\n",
        "random.shuffle(names)\n",
        "n1 = int(0.8 * len(names))\n",
        "n2 = int(0.9 * len(names))\n",
        "xs, ys = create_data(names, block_size)\n",
        "x_train, y_train = create_data(names[:n1], block_size)\n",
        "x_val, y_val = create_data(names[n1:n2], block_size)\n",
        "x_test, y_test = create_data(names[n2:], block_size)\n",
        "\n",
        "print(\"train -\", x_train.shape[0])\n",
        "print(\"val   -\", x_val.shape[0])\n",
        "print(\"test  -\", x_test.shape[0])\n",
        "print(\"total -\", xs.shape[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GsMkwrevgOZ9",
        "outputId": "b24753c7-f14e-415f-e793-04731018bb2f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train - 182705\n",
            "val   - 22652\n",
            "test  - 22789\n",
            "total - 228146\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for x, y in zip(x_train[:20], y_train[:20]):\n",
        "  print(''.join(itos[ix.item()] for ix in x), '-->', itos[y.item()])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmTTFvtxV_Iz",
        "outputId": "2678ca55-df9a-4957-fbf1-778220f03f6a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "........ --> c\n",
            ".......c --> a\n",
            "......ca --> r\n",
            ".....car --> l\n",
            "....carl --> o\n",
            "...carlo --> s\n",
            "..carlos --> .\n",
            "........ --> s\n",
            ".......s --> o\n",
            "......so --> l\n",
            ".....sol --> v\n",
            "....solv --> i\n",
            "...solvi --> .\n",
            "........ --> m\n",
            ".......m --> a\n",
            "......ma --> t\n",
            ".....mat --> y\n",
            "....maty --> l\n",
            "...matyl --> d\n",
            "..matyld --> a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# classes initializations\n",
        "\n",
        "class Linear:\n",
        "\n",
        "    def __init__(self, fan_in, fan_out, bias=True):\n",
        "        self.weight = torch.randn((fan_in, fan_out)) / fan_in**0.5\n",
        "        self.bias = torch.zeros((1, fan_out)) if bias else None\n",
        "\n",
        "    def __call__(self, x):\n",
        "        self.out = x @ self.weight\n",
        "        if self.bias is not None:\n",
        "            self.out += self.bias\n",
        "        return self.out\n",
        "\n",
        "    def parameters(self):\n",
        "        return [self.weight] + ([] if self.bias == None else [self.bias])\n",
        "\n",
        "\n",
        "class BatchNorm1D:\n",
        "\n",
        "    def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
        "        self.eps = eps # to avoid division by zero when variance is zero\n",
        "        self.momentum = momentum # the fraction of running mean and variance to be updated with new batch mean and variance\n",
        "        self.training = True # bn has different training and inference behaviour\n",
        "\n",
        "        # trainable parameters (trained with gradient update)\n",
        "        self.gamma = torch.ones(dim) # scaling parameter, init with ones because it is multiplied with normalized preactivations\n",
        "        self.beta = torch.zeros(dim) # shifting parameter, init with zeros because it is added with normalized preactivations\n",
        "\n",
        "        # buffers (trained with a running 'momentum update)\n",
        "        self.running_mean = torch.zeros(dim) # init with zero because it is subtracted from the preact values\n",
        "        self.running_var = torch.ones(dim) # init with ones because it is the denominator of the preact values\n",
        "\n",
        "    def __call__(self, x):\n",
        "        if self.training:\n",
        "            xmean = x.mean(0, keepdim=True)\n",
        "            xvar = x.var(0, keepdim=True)\n",
        "        else:\n",
        "            xmean = self.running_mean\n",
        "            xvar = self.running_var\n",
        "\n",
        "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to zero mean and unit standard deviation\n",
        "        self.out = self.gamma * xhat + self.beta\n",
        "\n",
        "        # update the buffers\n",
        "        if self.training:\n",
        "            with torch.no_grad():\n",
        "                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n",
        "                self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n",
        "\n",
        "        return self.out\n",
        "\n",
        "    def parameters(self):\n",
        "        return [self.gamma, self.beta]\n",
        "\n",
        "\n",
        "class Tanh:\n",
        "\n",
        "    def __call__(self, x):\n",
        "        self.out = torch.tanh(x)\n",
        "        return self.out\n",
        "\n",
        "    def parameters(self):\n",
        "        return []\n",
        "\n",
        "\n",
        "class Embedding:\n",
        "\n",
        "    def __init__(self, fan_in, fan_out):\n",
        "        self.W = torch.randn((fan_in, fan_out))\n",
        "\n",
        "    def __call__(self, x):\n",
        "        self.out = self.W[x]\n",
        "        return self.out\n",
        "\n",
        "    def parameters(self):\n",
        "        return [self.W]\n",
        "\n",
        "\n",
        "class Sequential:\n",
        "\n",
        "    def __init__(self, layers):\n",
        "        self.layers = layers\n",
        "        self.parameters = [p for layer in self.layers for p in layer.parameters()]\n",
        "        for p in self.parameters:\n",
        "            p.requires_grad = True\n",
        "\n",
        "    def __call__(self, x):\n",
        "        # print(list(x.shape), round(x.float().mean().item(), 2), round(x.float().std().item(), 2))\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "            # print(layer.__class__.__name__, \"                  \", list(x.shape), \"                \", round(x.std().item(), 2))\n",
        "        self.out = x\n",
        "        return self.out\n",
        "\n",
        "\n",
        "class FlattenConsecutive:\n",
        "\n",
        "    def __init__(self, n):\n",
        "        self.n = n\n",
        "\n",
        "    def __call__(self, x):\n",
        "        B, T, C = x.shape\n",
        "        self.out = x.view(B, T//self.n, C*self.n)\n",
        "        if T//self.n == 1:\n",
        "            self.out = torch.squeeze(self.out, 1)\n",
        "        return self.out\n",
        "\n",
        "    def parameters(self):\n",
        "        return []"
      ],
      "metadata": {
        "id": "RsEK8m3P3Zax"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MLP\n",
        "\n",
        "# for reproducibility\n",
        "torch.manual_seed(100)\n",
        "\n",
        "# hyperparameters\n",
        "n_embd = 10\n",
        "n_hidden = 100\n",
        "vocab_size = 27\n",
        "\n",
        "# model definition\n",
        "model = Sequential([\n",
        "    Embedding(vocab_size, n_embd),\n",
        "    Linear(                           n_embd, n_hidden, bias=False), BatchNorm1D(n_hidden), Tanh(),\n",
        "    FlattenConsecutive(2), Linear(2*n_hidden, n_hidden, bias=False), BatchNorm1D(n_hidden), Tanh(),\n",
        "    FlattenConsecutive(2), Linear(2*n_hidden, n_hidden, bias=False), BatchNorm1D(n_hidden), Tanh(),\n",
        "    FlattenConsecutive(2), Linear(2*n_hidden, n_hidden, bias=False), BatchNorm1D(n_hidden), Tanh(),\n",
        "    Linear(                         n_hidden, vocab_size, bias=False), BatchNorm1D(vocab_size)\n",
        "])\n",
        "\n",
        "# scaling of weights for better gradient propagation\n",
        "with torch.no_grad():\n",
        "    model.layers[-1].gamma *= 0.1 # last layer : make less confident\n",
        "    for layer in model.layers[:-1]: # all other layers : apply gain\n",
        "        if isinstance(layer, Linear):\n",
        "            layer.weight *= 5/3"
      ],
      "metadata": {
        "id": "hYZdaTWT5DRR"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_steps = 50001\n",
        "batch_size = 32\n",
        "lossi = []\n",
        "ud = []\n",
        "\n",
        "for i in range(max_steps):\n",
        "\n",
        "  # minibatch construct\n",
        "  ix = torch.randint(0, x_train.shape[0], (batch_size,))\n",
        "  Xb, Yb = x_train[ix], y_train[ix] # batch X,Y\n",
        "\n",
        "  # forward pass\n",
        "  logits = model(Xb)\n",
        "  loss = F.cross_entropy(logits, Yb) # loss function\n",
        "\n",
        "  # backward pass\n",
        "  for layer in model.layers:\n",
        "    layer.out.retain_grad() # AFTER_DEBUG: would take out retain_graph\n",
        "\n",
        "  for p in model.parameters:\n",
        "    p.grad = None\n",
        "  loss.backward()\n",
        "\n",
        "  # update\n",
        "  lr = 0.1 if i < 150000 else 0.01 # step learning rate decay\n",
        "  for p in model.parameters:\n",
        "    p.data += -lr * p.grad\n",
        "\n",
        "  # track stats\n",
        "  if i % 5000 == 0: # print every once in a while\n",
        "    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
        "  lossi.append(loss.log10().item())\n",
        "  with torch.no_grad():\n",
        "    ud.append([((lr*p.grad).std() / p.data.std()).log10().item() for p in model.parameters])\n",
        "\n",
        "#   if i >= 0:\n",
        "#     break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDXMJa65KjcH",
        "outputId": "d8241b5b-c1ea-41ed-b61d-4d4e574e6551"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      0/  50001: 3.2972\n",
            "   5000/  50001: 2.2684\n",
            "  10000/  50001: 1.9502\n",
            "  15000/  50001: 1.7524\n",
            "  20000/  50001: 2.0446\n",
            "  25000/  50001: 1.8163\n",
            "  30000/  50001: 1.9030\n",
            "  35000/  50001: 2.1476\n",
            "  40000/  50001: 1.9647\n",
            "  45000/  50001: 2.1701\n",
            "  50000/  50001: 2.4227\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluation\n",
        "\n",
        "@torch.no_grad()\n",
        "def split_loss(split):\n",
        "    x, y = {\n",
        "        \"all\": (xs, ys),\n",
        "        \"train\": (x_train, y_train),\n",
        "        \"val\": (x_val, y_val),\n",
        "        \"test\": (x_test, y_test)\n",
        "    }[split]\n",
        "    logits = model(x)\n",
        "    loss = F.cross_entropy(logits, y)\n",
        "    print(split, \"-\", loss.item())\n",
        "\n",
        "split_loss(\"val\")\n",
        "split_loss(\"all\")"
      ],
      "metadata": {
        "id": "DUzmfeM5gu1m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75ef164b-6590-40da-97f0-fc151d8961a6"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val - 2.1017796993255615\n",
            "all - 2.041135311126709\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# inference\n",
        "\n",
        "for layer in model.layers:\n",
        "    if isinstance(layer, BatchNorm1D):\n",
        "        layer.training = False\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in range(30):\n",
        "        context = [0] * block_size\n",
        "        while True:\n",
        "            logits = model(torch.tensor([context]))\n",
        "            probs = F.softmax(logits, dim=1)\n",
        "            ix = torch.multinomial(probs, num_samples=1).item()\n",
        "            context = context[1:] + [ix]\n",
        "            print(itos[ix], end=\"\")\n",
        "            if ix == 0:\n",
        "                break\n",
        "        print(\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8R1ZSu-hLzB",
        "outputId": "5d9a8fc2-7a13-497d-e32d-93e212bb084c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "yania.\n",
            "yathi.\n",
            "conte.\n",
            "manai.\n",
            "argedan.\n",
            "amaelin.\n",
            "jianna.\n",
            "oluusah.\n",
            "yush.\n",
            "lekhan.\n",
            "carson.\n",
            "toniah.\n",
            "chanay.\n",
            "mihi.\n",
            "keania.\n",
            "silver.\n",
            "sergaria.\n",
            "garren.\n",
            "samara.\n",
            "mannalyna.\n",
            "aynery.\n",
            "iziya.\n",
            "miccord.\n",
            "izekaria.\n",
            "pereis.\n",
            "keyte.\n",
            "eleutabfe.\n",
            "tagiain.\n",
            "ohukah.\n",
            "ezeionyose.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l-eXj5twAVtJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}